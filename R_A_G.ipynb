{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EbubeTheGoat/R.A.G/blob/main/R_A_G.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGVx2Suyhz-U"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsJwnrG4TkhA"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "hf_token = userdata.get(\"HF_TOKEN\")\n",
        "login(hf_token,add_to_git_credential = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64S-iXB0hz4B"
      },
      "outputs": [],
      "source": [
        "document = \"/content/200l.pdf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "JGB3qm0oLjwo"
      },
      "outputs": [],
      "source": [
        "!pip install pdfplumber\n",
        "!pip install bitsandbytes accelerate\n",
        "!pip install langchain-openai\n",
        "!pip install langchain-chroma\n",
        "!pip install langchain-huggingface\n",
        "import pdfplumber\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import json\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import SystemMessage, HumanMessage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Ta5ZcNICb8Pg"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "pdf_path = document\n",
        "all_names = []\n",
        "\n",
        "with pdfplumber.open(pdf_path) as pdf:\n",
        "    for page in pdf.pages:\n",
        "        text = page.extract_table()\n",
        "        if text:\n",
        "            for row in text:\n",
        "                if row and len(row) > 1:  # Check row exists and has enough columns\n",
        "                    name = row[1]\n",
        "                    if name and name != \"NAMES\" and not name.startswith(\"FACULTY\"):\n",
        "                        clean_name = name.replace(\",\", \"\").strip().title()\n",
        "                        all_names.append(clean_name)\n",
        "\n",
        "print(f\"Extracted {len(all_names)} names\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_H-SvQ3WcGbf"
      },
      "outputs": [],
      "source": [
        "# Process in smaller batches to avoid context length issues\n",
        "\n",
        "\n",
        "\n",
        "Model = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(Model)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    Model,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\",\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3YBf_3tuGnLH"
      },
      "outputs": [],
      "source": [
        "all_data = []\n",
        "batch_size = 10\n",
        "\n",
        "for i in range(0, len(all_names), batch_size):\n",
        "\n",
        "    batch_names = all_names[i:i+batch_size]\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You create synthetic data for pharmacy professionals. Return ONLY valid JSON with no additional text.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"\"\"Create synthetic data for these pharmacists. Return as JSON array with this exact structure:\n",
        "[\n",
        "  {{\n",
        "    name: Full Name,\n",
        "    favorite_course: choose between 'CPM282','PCL451','PCH511','PMB442',\n",
        "    expertise: choose between 'Oncology' , 'Family Medicine','HIV/AIDS and Tuberculosis',\n",
        "    age: number between 20-35,\n",
        "    gender: 'Male' or 'Female',\n",
        "    marital_status: choose between 'Single','Married',\n",
        "    favorite_sport: choose between 'Football','Basketball','Volleyball','Olympics'\n",
        "  }}\n",
        "]\n",
        "\n",
        "Names: {batch_names}\n",
        "\n",
        "Return ONLY the JSON array, no other text.\"\"\"}\n",
        "    ]\n",
        "\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=2048,\n",
        "        add_generation_prompt=True\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            inputs,\n",
        "            max_new_tokens=2048,  # Increased for more names\n",
        "            do_sample=True,  # Enable sampling for variety\n",
        "            temperature=0.7,  # Add some randomness\n",
        "            top_p=0.9\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(output[0][len(inputs[0]):], skip_special_tokens=True)\n",
        "    print(f\"\\nBatch {i//batch_size + 1} response:\\n{response}\\n\")\n",
        "\n",
        "    # Try to parse JSON\n",
        "    try:\n",
        "        batch_data = json.loads(response)\n",
        "        all_data.extend(batch_data)\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Failed to parse JSON for batch {i//batch_size + 1}\")\n",
        "\n",
        "print(f\"\\nTotal records created: {len(all_data)}\")\n",
        "print(json.dumps(all_data, indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NvjFO6Uqr4S"
      },
      "outputs": [],
      "source": [
        "all_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "FRByZxeDzNWS"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-text-splitters\n",
        "from dotenv import load_dotenv\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Je3dAG-Jpl8q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "MODEL = \"gpt-4.1-nano\"\n",
        "db_name = \"vector_db\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "if openai_api_key:\n",
        "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
        "else:\n",
        "    print(\"OpenAI API Key not set\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUG16wZ6weQJ"
      },
      "outputs": [],
      "source": [
        "print(all_data[0].keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxLWwMpWvP5M"
      },
      "outputs": [],
      "source": [
        "from langchain_core.documents import Document\n",
        "docs= []\n",
        "for d in all_data:\n",
        "  combined_text = \", \".join([f\"{key}:{value}\" for key, value in d.items()])\n",
        "  docs.append(Document(page_content=combined_text,metadata={\"name\":d.get(\"name\")}))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import MarkdownTextSplitter"
      ],
      "metadata": {
        "id": "WglA9BuZDb5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCAJQ9OU0cnM"
      },
      "outputs": [],
      "source": [
        "text_splitter = MarkdownTextSplitter(chunk_size = 120,chunk_overlap= 60)\n",
        "chunk = text_splitter.split_documents(docs)\n",
        "print(f\"Divided into {len(chunk)} chunks\")\n",
        "print(f\"first chunk: \\n\\n {chunk[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXLY9q7pqqF1"
      },
      "outputs": [],
      "source": [
        "model_path = \"/content/drive/MyDrive/pharmacist_model\"\n",
        "model.save_pretrained(model_path)\n",
        "tokenizer.save_pretrained(model_path)\n",
        "print(f'Model saved to {model_path}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmS2FpR91_Qz"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "chunks_path = \"/content/drive/MyDrive/chunks.pkl\"\n",
        "with open(chunks_path, 'wb') as f:\n",
        "    pickle.dump(chunk, f)\n",
        "print(\"Chunks saved successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rym1C8PMLZU"
      },
      "outputs": [],
      "source": [
        "model_path = \"/content/drive/MyDrive/pharmacist_model\"\n",
        "tokenizer=AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path,device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9n1uSV0IMr6E"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "chunks_path = \"/content/drive/MyDrive/chunks.pkl\"\n",
        "with open(chunks_path, 'rb') as f:\n",
        "    chunk = pickle.load(f)\n",
        "print(f\"loaded {len(chunk)} chunks\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YicqaE7iN6r-"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-huggingface\n",
        "!pip install langchain-chroma\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_chroma import Chroma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "dMjTQcLEOzq-"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-openai\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgG_d6FL3qdB"
      },
      "outputs": [],
      "source": [
        "\n",
        "#embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "embeddings =OpenAIEmbeddings(model = \"text-embedding-3-large\")\n",
        "\n",
        "if os.path.exists(db_name):\n",
        "  Chroma(persist_directory=db_name,embedding_function=embeddings).delete_collection()\n",
        "vectorstore = Chroma.from_documents(chunk,embedding=embeddings,persist_directory=db_name)\n",
        "print(f\"Vectorstore created with {vectorstore._collection.count()} documents \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Faik0l8_6lBV"
      },
      "outputs": [],
      "source": [
        "collection = vectorstore._collection\n",
        "count = collection.count()\n",
        "\n",
        "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
        "dimensions = len(sample_embedding)\n",
        "print(f\"There are {count:,} vectors with {dimensions:,} dimensions in the vector store\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKzNMRQF6-XO"
      },
      "outputs": [],
      "source": [
        "retriever = vectorstore.as_retriever()\n",
        "llm = ChatOpenAI(temperature=0, model_name=MODEL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Be7ijyhpQO6E"
      },
      "outputs": [],
      "source": [
        "retriever.invoke(\"Who is George Ebubechukwu?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJKaebdiQldl"
      },
      "outputs": [],
      "source": [
        "llm.invoke(\"Who is George Ebubechukwu?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tqlKCZld7Em"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from tqdm import tqdm\n",
        "from openai import OpenAI # For a progress bar\n",
        "\n",
        "# 1. Setup API Key and Model (Replace with your actual key name)\n",
        "# Ensure you have 'OPENAI_API_KEY' in your Colab Secrets (üîë icon)\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "MODEL = \"gpt-4.1-nano\"\n",
        "client = OpenAI()\n",
        "\n",
        "def generate_eval_record(person):\n",
        "    \"\"\"Prompts the LLM to create a diverse question/answer pair.\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    You are creating a diverse evaluation dataset for a pharmacist profile system.\n",
        "    Using the profile below, generate ONE unique question/answer pair.\n",
        "\n",
        "    Profile:\n",
        "    - Name: {person['name']}\n",
        "    - Expertise: {person['expertise']}\n",
        "    - Age: {person['age']}\n",
        "    - Marital Status: {person['marital_status']}\n",
        "    - Favorite Course: {person['favorite_course']}\n",
        "    - Favorite Sport: {person['favorite_sport']}\n",
        "\n",
        "    Randomly pick ONE of these categories for the question:\n",
        "    1. PROFESSIONAL: Ask about their career path or favorite academic course.\n",
        "    2. PERSONAL: Ask about their hobbies, sport, or family status.\n",
        "    3. HYPOTHETICAL: Ask how they might use their expertise in a specific pharmacy scenario.\n",
        "    4. COMPARATIVE: Ask to compare their age or expertise to a typical professional role.\n",
        "\n",
        "    You MUST randomly choose from all the categories above.Do not skew it to any two categories.\n",
        "    All categories must be present\n",
        "\n",
        "    Output format MUST be a single line JSON with no markdown formatting:\n",
        "    {{\"question\": \"...\", \"keywords\": [\"word1\", \"word2\"], \"reference_answer\": \"...\", \"category\": \"...\"}}\n",
        "    \"\"\"\n",
        "\n",
        "    # Replace this block with your model's specific calling method\n",
        "    # Example using OpenAI-style client:\n",
        "    response = client.chat.completions.create(model=MODEL, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
        "    return json.loads(response.choices[0].message.content)\n",
        "\n",
        "    # Simulation for demonstration:\n",
        "\n",
        "# 2. Process all data and save to JSONL\n",
        "output_filename = \"diverse_tests.jsonl\"\n",
        "\n",
        "with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "    for person in tqdm(all_data, desc=\"Generating Dataset\"):\n",
        "        try:\n",
        "            # Generate the record\n",
        "            eval_record = generate_eval_record(person)\n",
        "\n",
        "            # Write exactly one JSON object per line\n",
        "            f.write(json.dumps(eval_record) + \"\\n\")\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping record for {person['name']} due to error: {e}\")\n",
        "\n",
        "print(f\"\\nSuccessfully created {output_filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYY_9Hl2L_Ro"
      },
      "outputs": [],
      "source": [
        "# View the first 5 lines of the file\n",
        "with open(\"diverse_tests.jsonl\", \"r\") as f:\n",
        "    for i, line in enumerate(f):\n",
        "        if i < 5:\n",
        "            print(line.strip())\n",
        "        else:\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsP7WZColsGT"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "TEST_FILE = str(Path.cwd()/ \"diverse_tests.jsonl\")\n",
        "\n",
        "\n",
        "class TestQuestion(BaseModel):\n",
        "    \"\"\"A test question with expected keywords and reference answer.\"\"\"\n",
        "\n",
        "    question: str = Field(description=\"The question to ask the RAG system\")\n",
        "    keywords: list[str] = Field(description=\"Keywords that must appear in retrieved context\")\n",
        "    reference_answer: str = Field(description=\"The reference answer for this question\")\n",
        "    category: str = Field(description=\"Question category (e.g., direct_fact, spanning, temporal)\")\n",
        "\n",
        "\n",
        "def load_tests() -> list[TestQuestion]:\n",
        "    \"\"\"Load test questions from JSONL file.\"\"\"\n",
        "    tests = []\n",
        "    with open(TEST_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            data = json.loads(line.strip())\n",
        "            tests.append(TestQuestion(**data))\n",
        "    return tests\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlpdG3E9Ppjg"
      },
      "outputs": [],
      "source": [
        "tests = load_tests()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQGmeEDzQClb"
      },
      "outputs": [],
      "source": [
        "len(tests )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGKgKyZUVelN"
      },
      "outputs": [],
      "source": [
        "example = tests[0]\n",
        "print(example.question)\n",
        "print(example.category)\n",
        "print(example.reference_answer)\n",
        "print(example.keywords)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4ghFsXiWffn"
      },
      "outputs": [],
      "source": [
        "\n",
        "from collections import Counter\n",
        "count = Counter([t.category for t in tests])\n",
        "count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yE_kl9XT2jKp"
      },
      "outputs": [],
      "source": [
        "from langchain_core.documents import Document\n",
        "Retriever = 5\n",
        "def fetch_context(question: str) -> list[Document]:\n",
        "    \"\"\"\n",
        "    Retrieve relevant context documents for a question.\n",
        "    \"\"\"\n",
        "    return retriever.invoke(question, k=Retriever)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A216Tyht7V03"
      },
      "outputs": [],
      "source": [
        "\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are a knowledgeable, friendly assistant who knows  about pharmacists,hobbies and expertise a lot.\n",
        "You are chatting with a user about the pharmacist's life.\n",
        "If relevant, use the given context to answer any question.\n",
        "IF YOU DON'T KNOW THE ANSWER SAY SO.\n",
        "Context:\n",
        "{context}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YahXBKUP4Og3"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, convert_to_messages\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "def combined_question(question: str, history: list[dict] = []) -> str:\n",
        "    \"\"\"\n",
        "    Combine all the user's messages into a single string.\n",
        "    \"\"\"\n",
        "    prior = \"\\n\".join(m[\"content\"] for m in history if m[\"role\"] == \"user\")\n",
        "    return prior + \"\\n\" + question\n",
        "\n",
        "\n",
        "def answer_question(question: str, history: list[dict] = []) -> tuple[str, list[Document]]:\n",
        "    \"\"\"\n",
        "    Answer the given question with RAG; return the answer and the context documents.\n",
        "    \"\"\"\n",
        "    combined = combined_question(question, history)\n",
        "    docs = fetch_context(combined)\n",
        "    context = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "    system_prompt = SYSTEM_PROMPT.format(context=context)\n",
        "    messages = [SystemMessage(content=system_prompt)]\n",
        "    messages.extend(convert_to_messages(history))\n",
        "    messages.append(HumanMessage(content=question))\n",
        "    response = llm.invoke(messages)\n",
        "    return response.content, docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6vW9Q20-WqIx"
      },
      "outputs": [],
      "source": [
        "!pip install litellm\n",
        "import sys\n",
        "import math\n",
        "from pydantic import BaseModel, Field\n",
        "from litellm import completion\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "\n",
        "\n",
        "MODEL = \"gpt-4.1-nano\"\n",
        "db_name = \"vector_db\"\n",
        "\n",
        "\n",
        "class RetrievalEval(BaseModel):\n",
        "    \"\"\"Evaluation metrics for retrieval performance.\"\"\"\n",
        "\n",
        "    mrr: float = Field(description=\"Mean Reciprocal Rank - average across all keywords\")\n",
        "    ndcg: float = Field(description=\"Normalized Discounted Cumulative Gain (binary relevance)\")\n",
        "    keywords_found: int = Field(description=\"Number of keywords found in top-k results\")\n",
        "    total_keywords: int = Field(description=\"Total number of keywords to find\")\n",
        "    keyword_coverage: float = Field(description=\"Percentage of keywords found\")\n",
        "\n",
        "\n",
        "class AnswerEval(BaseModel):\n",
        "    \"\"\"LLM-as-a-judge evaluation of answer quality.\"\"\"\n",
        "\n",
        "    feedback: str = Field(\n",
        "        description=\"Concise feedback on the answer quality, comparing it to the reference answer and evaluating based on the retrieved context\"\n",
        "    )\n",
        "    accuracy: float = Field(\n",
        "        description=\"How factually correct is the answer compared to the reference answer? 1 (wrong. any wrong answer must score 1) to 5 (ideal - perfectly accurate). An acceptable answer would score 3.\"\n",
        "    )\n",
        "    completeness: float = Field(\n",
        "        description=\"How complete is the answer in addressing all aspects of the question? 1 (very poor - missing key information) to 5 (ideal - all the information from the reference answer is provided completely). Only answer 5 if ALL information from the reference answer is included.\"\n",
        "    )\n",
        "    relevance: float = Field(\n",
        "        description=\"How relevant is the answer to the specific question asked? 1 (very poor - off-topic) to 5 (ideal - directly addresses question and gives no additional information). Only answer 5 if the answer is completely relevant to the question and gives no additional information.\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-QW63-mXcge"
      },
      "outputs": [],
      "source": [
        "def calculate_mrr(keyword: str, retrieved_docs: list) -> float:\n",
        "    \"\"\"Calculate reciprocal rank for a single keyword (case-insensitive).\"\"\"\n",
        "    keyword_lower = keyword.lower()\n",
        "    for rank, doc in enumerate(retrieved_docs, start=1):\n",
        "        if keyword_lower in doc.page_content.lower():\n",
        "            return 1.0 / rank\n",
        "    return 0.0\n",
        "\n",
        "\n",
        "def calculate_dcg(relevances: list[int], k: int) -> float:\n",
        "    \"\"\"Calculate Discounted Cumulative Gain.\"\"\"\n",
        "    dcg = 0.0\n",
        "    for i in range(min(k, len(relevances))):\n",
        "        dcg += relevances[i] / math.log2(i + 2)  # i+2 because rank starts at 1\n",
        "    return dcg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8HUJG-UZT6d"
      },
      "outputs": [],
      "source": [
        "def calculate_ndcg(keyword: str, retrieved_docs: list, k: int = 10) -> float:\n",
        "    \"\"\"Calculate nDCG for a single keyword (binary relevance, case-insensitive).\"\"\"\n",
        "    keyword_lower = keyword.lower()\n",
        "\n",
        "    # Binary relevance: 1 if keyword found, 0 otherwise\n",
        "    relevances = [\n",
        "        1 if keyword_lower in doc.page_content.lower() else 0 for doc in retrieved_docs[:k]\n",
        "    ]\n",
        "\n",
        "    # DCG\n",
        "    dcg = calculate_dcg(relevances, k)\n",
        "\n",
        "    # Ideal DCG (best case: keyword in first position)\n",
        "    ideal_relevances = sorted(relevances, reverse=True)\n",
        "    idcg = calculate_dcg(ideal_relevances, k)\n",
        "\n",
        "    return dcg / idcg if idcg > 0 else 0.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrrCAxQNZbzI"
      },
      "outputs": [],
      "source": [
        "def evaluate_retrieval(test: TestQuestion, k: int = 10) -> RetrievalEval:\n",
        "    \"\"\"\n",
        "    Evaluate retrieval performance for a test question.\n",
        "\n",
        "    Args:\n",
        "        test: TestQuestion object containing question and keywords\n",
        "        k: Number of top documents to retrieve (default 10)\n",
        "\n",
        "    Returns:\n",
        "        RetrievalEval object with MRR, nDCG, and keyword coverage metrics\n",
        "    \"\"\"\n",
        "    # Retrieve documents using shared answer module\n",
        "    retrieved_docs = fetch_context(test.question)\n",
        "\n",
        "    # Calculate MRR (average across all keywords)\n",
        "    mrr_scores = [calculate_mrr(keyword, retrieved_docs) for keyword in test.keywords]\n",
        "    avg_mrr = sum(mrr_scores) / len(mrr_scores) if mrr_scores else 0.0\n",
        "\n",
        "    # Calculate nDCG (average across all keywords)\n",
        "    ndcg_scores = [calculate_ndcg(keyword, retrieved_docs, k) for keyword in test.keywords]\n",
        "    avg_ndcg = sum(ndcg_scores) / len(ndcg_scores) if ndcg_scores else 0.0\n",
        "\n",
        "    # Calculate keyword coverage\n",
        "    keywords_found = sum(1 for score in mrr_scores if score > 0)\n",
        "    total_keywords = len(test.keywords)\n",
        "    keyword_coverage = (keywords_found / total_keywords * 100) if total_keywords > 0 else 0.0\n",
        "\n",
        "    return RetrievalEval(\n",
        "        mrr=avg_mrr,\n",
        "        ndcg=avg_ndcg,\n",
        "        keywords_found=keywords_found,\n",
        "        total_keywords=total_keywords,\n",
        "        keyword_coverage=keyword_coverage,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdPxfKx0ZiGS"
      },
      "outputs": [],
      "source": [
        "def evaluate_answer(test: TestQuestion) -> tuple[AnswerEval, str, list]:\n",
        "    \"\"\"\n",
        "    Evaluate answer quality using LLM-as-a-judge (async).\n",
        "\n",
        "    Args:\n",
        "        test: TestQuestion object containing question and reference answer\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (AnswerEval object, generated_answer string, retrieved_docs list)\n",
        "    \"\"\"\n",
        "    # Get RAG response using shared answer module\n",
        "    generated_answer, retrieved_docs = answer_question(test.question)\n",
        "\n",
        "    # LLM judge prompt\n",
        "    judge_messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are an expert evaluator assessing the quality of answers. Evaluate the generated answer by comparing it to the reference answer. Only give 5/5 scores for perfect answers.\",\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"Question:\n",
        "{test.question}\n",
        "\n",
        "Generated Answer:\n",
        "{generated_answer}\n",
        "\n",
        "Reference Answer:\n",
        "{test.reference_answer}\n",
        "\n",
        "Please evaluate the generated answer on three dimensions:\n",
        "1. Accuracy: How factually correct is it compared to the reference answer? Only give 5/5 scores for perfect answers.\n",
        "2. Completeness: How thoroughly does it address all aspects of the question, covering all the information from the reference answer?\n",
        "3. Relevance: How well does it directly answer the specific question asked, giving no additional information?\n",
        "\n",
        "Provide detailed feedback and scores from 1 (very poor) to 5 (ideal) for each dimension. If the answer is wrong, then the accuracy score must be 1.\"\"\",\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    # Call LLM judge with structured outputs (async)\n",
        "    judge_response = completion(model=MODEL, messages=judge_messages, response_format=AnswerEval)\n",
        "\n",
        "    answer_eval = AnswerEval.model_validate_json(judge_response.choices[0].message.content)\n",
        "\n",
        "    return answer_eval, generated_answer, retrieved_docs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MSlIhM3ZrvP"
      },
      "outputs": [],
      "source": [
        "def evaluate_all_retrieval():\n",
        "    \"\"\"Evaluate all retrieval tests.\"\"\"\n",
        "    tests = load_tests()\n",
        "    total_tests = len(tests)\n",
        "    for index, test in enumerate(tests):\n",
        "        result = evaluate_retrieval(test)\n",
        "        progress = (index + 1) / total_tests\n",
        "        yield test, result, progress\n",
        "\n",
        "\n",
        "def evaluate_all_answers():\n",
        "    \"\"\"Evaluate all answers to tests using batched async execution.\"\"\"\n",
        "    tests = load_tests()\n",
        "    total_tests = len(tests)\n",
        "    for index, test in enumerate(tests):\n",
        "        result = evaluate_answer(test)[0]\n",
        "        progress = (index + 1) / total_tests\n",
        "        yield test, result, progress\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OgPny3PZ19u"
      },
      "outputs": [],
      "source": [
        "def run_cli_evaluation(test_number: int):\n",
        "    \"\"\"Run evaluation for a specific test (async helper for CLI).\"\"\"\n",
        "    # Load tests\n",
        "    tests = load_tests(\"tests.jsonl\")\n",
        "\n",
        "    if test_number < 0 or test_number >= len(tests):\n",
        "        print(f\"Error: test_row_number must be between 0 and {len(tests) - 1}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Get the test\n",
        "    test = tests[test_number]\n",
        "\n",
        "    # Print test info\n",
        "    print(f\"\\n{'=' * 80}\")\n",
        "    print(f\"Test #{test_number}\")\n",
        "    print(f\"{'=' * 80}\")\n",
        "    print(f\"Question: {test.question}\")\n",
        "    print(f\"Keywords: {test.keywords}\")\n",
        "    print(f\"Category: {test.category}\")\n",
        "    print(f\"Reference Answer: {test.reference_answer}\")\n",
        "\n",
        "    # Retrieval Evaluation\n",
        "    print(f\"\\n{'=' * 80}\")\n",
        "    print(\"Retrieval Evaluation\")\n",
        "    print(f\"{'=' * 80}\")\n",
        "\n",
        "    retrieval_result = evaluate_retrieval(test)\n",
        "\n",
        "    print(f\"MRR: {retrieval_result.mrr:.4f}\")\n",
        "    print(f\"nDCG: {retrieval_result.ndcg:.4f}\")\n",
        "    print(f\"Keywords Found: {retrieval_result.keywords_found}/{retrieval_result.total_keywords}\")\n",
        "    print(f\"Keyword Coverage: {retrieval_result.keyword_coverage:.1f}%\")\n",
        "\n",
        "    # Answer Evaluation\n",
        "    print(f\"\\n{'=' * 80}\")\n",
        "    print(\"Answer Evaluation\")\n",
        "    print(f\"{'=' * 80}\")\n",
        "\n",
        "    answer_result, generated_answer, retrieved_docs = evaluate_answer(test)\n",
        "\n",
        "    print(f\"\\nGenerated Answer:\\n{generated_answer}\")\n",
        "    print(f\"\\nFeedback:\\n{answer_result.feedback}\")\n",
        "    print(\"\\nScores:\")\n",
        "    print(f\"  Accuracy: {answer_result.accuracy:.2f}/5\")\n",
        "    print(f\"  Completeness: {answer_result.completeness:.2f}/5\")\n",
        "    print(f\"  Relevance: {answer_result.relevance:.2f}/5\")\n",
        "    print(f\"\\n{'=' * 80}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjHHcUeV9j4E"
      },
      "outputs": [],
      "source": [
        "evaluate_retrieval(example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOhEWwfx-n72"
      },
      "outputs": [],
      "source": [
        "eval,answer,chunks = evaluate_answer(example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBDA9q1p_igY"
      },
      "outputs": [],
      "source": [
        "eval,answer,chunks = evaluate_answer(example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGoMXNpW_sB8"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv(override=True)\n",
        "\n",
        "# Color coding thresholds - Retrieval\n",
        "MRR_GREEN = 0.9\n",
        "MRR_AMBER = 0.75\n",
        "NDCG_GREEN = 0.9\n",
        "NDCG_AMBER = 0.75\n",
        "COVERAGE_GREEN = 90.0\n",
        "COVERAGE_AMBER = 75.0\n",
        "\n",
        "# Color coding thresholds - Answer (1-5 scale)\n",
        "ANSWER_GREEN = 4.5\n",
        "ANSWER_AMBER = 4.0\n",
        "\n",
        "\n",
        "def get_color(value: float, metric_type: str) -> str:\n",
        "    \"\"\"Get color based on metric value and type.\"\"\"\n",
        "    if metric_type == \"mrr\":\n",
        "        if value >= MRR_GREEN:\n",
        "            return \"green\"\n",
        "        elif value >= MRR_AMBER:\n",
        "            return \"orange\"\n",
        "        else:\n",
        "            return \"red\"\n",
        "    elif metric_type == \"ndcg\":\n",
        "        if value >= NDCG_GREEN:\n",
        "            return \"green\"\n",
        "        elif value >= NDCG_AMBER:\n",
        "            return \"orange\"\n",
        "        else:\n",
        "            return \"red\"\n",
        "    elif metric_type == \"coverage\":\n",
        "        if value >= COVERAGE_GREEN:\n",
        "            return \"green\"\n",
        "        elif value >= COVERAGE_AMBER:\n",
        "            return \"orange\"\n",
        "        else:\n",
        "            return \"red\"\n",
        "    elif metric_type in [\"accuracy\", \"completeness\", \"relevance\"]:\n",
        "        if value >= ANSWER_GREEN:\n",
        "            return \"green\"\n",
        "        elif value >= ANSWER_AMBER:\n",
        "            return \"orange\"\n",
        "        else:\n",
        "            return \"red\"\n",
        "    return \"black\"\n",
        "\n",
        "\n",
        "def format_metric_html(\n",
        "    label: str,\n",
        "    value: float,\n",
        "    metric_type: str,\n",
        "    is_percentage: bool = False,\n",
        "    score_format: bool = False,\n",
        ") -> str:\n",
        "    \"\"\"Format a metric with color coding.\"\"\"\n",
        "    color = get_color(value, metric_type)\n",
        "    if is_percentage:\n",
        "        value_str = f\"{value:.1f}%\"\n",
        "    elif score_format:\n",
        "        value_str = f\"{value:.2f}/5\"\n",
        "    else:\n",
        "        value_str = f\"{value:.4f}\"\n",
        "    return f\"\"\"\n",
        "    <div style=\"margin: 10px 0; padding: 15px; background-color: #f5f5f5; border-radius: 8px; border-left: 5px solid {color};\">\n",
        "        <div style=\"font-size: 14px; color: #666; margin-bottom: 5px;\">{label}</div>\n",
        "        <div style=\"font-size: 28px; font-weight: bold; color: {color};\">{value_str}</div>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "def run_retrieval_evaluation(progress=gr.Progress()):\n",
        "    \"\"\"Run retrieval evaluation and yield updates.\"\"\"\n",
        "    total_mrr = 0.0\n",
        "    total_ndcg = 0.0\n",
        "    total_coverage = 0.0\n",
        "    category_mrr = defaultdict(list)\n",
        "    count = 0\n",
        "\n",
        "    for test, result, prog_value in evaluate_all_retrieval():\n",
        "        count += 1\n",
        "        total_mrr += result.mrr\n",
        "        total_ndcg += result.ndcg\n",
        "        total_coverage += result.keyword_coverage\n",
        "\n",
        "        category_mrr[test.category].append(result.mrr)\n",
        "\n",
        "        # Update progress bar only\n",
        "        progress(prog_value, desc=f\"Evaluating test {count}...\")\n",
        "\n",
        "    # Calculate final averages\n",
        "    avg_mrr = total_mrr / count\n",
        "    avg_ndcg = total_ndcg / count\n",
        "    avg_coverage = total_coverage / count\n",
        "\n",
        "    # Create final summary metrics HTML\n",
        "    final_html = f\"\"\"\n",
        "    <div style=\"padding: 0;\">\n",
        "        {format_metric_html(\"Mean Reciprocal Rank (MRR)\", avg_mrr, \"mrr\")}\n",
        "        {format_metric_html(\"Normalized DCG (nDCG)\", avg_ndcg, \"ndcg\")}\n",
        "        {format_metric_html(\"Keyword Coverage\", avg_coverage, \"coverage\", is_percentage=True)}\n",
        "        <div style=\"margin-top: 20px; padding: 10px; background-color: #d4edda; border-radius: 5px; text-align: center; border: 1px solid #c3e6cb;\">\n",
        "            <span style=\"font-size: 14px; color: #155724; font-weight: bold;\">‚úì Evaluation Complete: {count} tests</span>\n",
        "        </div>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "    # Create final bar chart data\n",
        "    category_data = []\n",
        "    for category, mrr_scores in category_mrr.items():\n",
        "        avg_cat_mrr = sum(mrr_scores) / len(mrr_scores)\n",
        "        category_data.append({\"Category\": category, \"Average MRR\": avg_cat_mrr})\n",
        "\n",
        "    df = pd.DataFrame(category_data)\n",
        "\n",
        "    return final_html, df\n",
        "\n",
        "\n",
        "def run_answer_evaluation(progress=gr.Progress()):\n",
        "    \"\"\"Run answer evaluation and yield updates (async).\"\"\"\n",
        "    total_accuracy = 0.0\n",
        "    total_completeness = 0.0\n",
        "    total_relevance = 0.0\n",
        "    category_accuracy = defaultdict(list)\n",
        "    count = 0\n",
        "\n",
        "    for test, result, prog_value in evaluate_all_answers():\n",
        "        count += 1\n",
        "        total_accuracy += result.accuracy\n",
        "        total_completeness += result.completeness\n",
        "        total_relevance += result.relevance\n",
        "\n",
        "        category_accuracy[test.category].append(result.accuracy)\n",
        "\n",
        "        # Update progress bar only\n",
        "        progress(prog_value, desc=f\"Evaluating test {count}...\")\n",
        "\n",
        "    # Calculate final averages\n",
        "    avg_accuracy = total_accuracy / count\n",
        "    avg_completeness = total_completeness / count\n",
        "    avg_relevance = total_relevance / count\n",
        "\n",
        "    # Create final summary metrics HTML\n",
        "    final_html = f\"\"\"\n",
        "    <div style=\"padding: 0;\">\n",
        "        {format_metric_html(\"Accuracy\", avg_accuracy, \"accuracy\", score_format=True)}\n",
        "        {format_metric_html(\"Completeness\", avg_completeness, \"completeness\", score_format=True)}\n",
        "        {format_metric_html(\"Relevance\", avg_relevance, \"relevance\", score_format=True)}\n",
        "        <div style=\"margin-top: 20px; padding: 10px; background-color: #d4edda; border-radius: 5px; text-align: center; border: 1px solid #c3e6cb;\">\n",
        "            <span style=\"font-size: 14px; color: #155724; font-weight: bold;\">‚úì Evaluation Complete: {count} tests</span>\n",
        "        </div>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "    # Create final bar chart data\n",
        "    category_data = []\n",
        "    for category, accuracy_scores in category_accuracy.items():\n",
        "        avg_cat_accuracy = sum(accuracy_scores) / len(accuracy_scores)\n",
        "        category_data.append({\"Category\": category, \"Average Accuracy\": avg_cat_accuracy})\n",
        "\n",
        "    df = pd.DataFrame(category_data)\n",
        "\n",
        "    return final_html, df\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Launch the Gradio evaluation app.\"\"\"\n",
        "    theme = gr.themes.Soft(font=[\"Inter\", \"system-ui\", \"sans-serif\"])\n",
        "\n",
        "    with gr.Blocks(title=\"RAG Evaluation Dashboard\", theme=theme) as app:\n",
        "        gr.Markdown(\"# üìä RAG Evaluation Dashboard\")\n",
        "        gr.Markdown(\"Evaluate retrieval and answer quality for the Insurellm RAG system\")\n",
        "\n",
        "        # RETRIEVAL SECTION\n",
        "        gr.Markdown(\"## üîç Retrieval Evaluation\")\n",
        "\n",
        "        retrieval_button = gr.Button(\"Run Evaluation\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                retrieval_metrics = gr.HTML(\n",
        "                    \"<div style='padding: 20px; text-align: center; color: #999;'>Click 'Run Evaluation' to start</div>\"\n",
        "                )\n",
        "\n",
        "            with gr.Column(scale=1):\n",
        "                retrieval_chart = gr.BarPlot(\n",
        "                    x=\"Category\",\n",
        "                    y=\"Average MRR\",\n",
        "                    title=\"Average MRR by Category\",\n",
        "                    y_lim=[0, 1],\n",
        "                    height=400,\n",
        "                )\n",
        "\n",
        "        # ANSWERING SECTION\n",
        "        gr.Markdown(\"## üí¨ Answer Evaluation\")\n",
        "\n",
        "        answer_button = gr.Button(\"Run Evaluation\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                answer_metrics = gr.HTML(\n",
        "                    \"<div style='padding: 20px; text-align: center; color: #999;'>Click 'Run Evaluation' to start</div>\"\n",
        "                )\n",
        "\n",
        "            with gr.Column(scale=1):\n",
        "                answer_chart = gr.BarPlot(\n",
        "                    x=\"Category\",\n",
        "                    y=\"Average Accuracy\",\n",
        "                    title=\"Average Accuracy by Category\",\n",
        "                    y_lim=[1, 5],\n",
        "                    height=400,\n",
        "                )\n",
        "\n",
        "        # Wire up the evaluations\n",
        "        retrieval_button.click(\n",
        "            fn=run_retrieval_evaluation,\n",
        "            outputs=[retrieval_metrics, retrieval_chart],\n",
        "        )\n",
        "\n",
        "        answer_button.click(\n",
        "            fn=run_answer_evaluation,\n",
        "            outputs=[answer_metrics, answer_chart],\n",
        "        )\n",
        "\n",
        "    app.launch(inbrowser=True)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0Sw0LKfQV6v"
      },
      "outputs": [],
      "source": [
        "w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VKnK5S_EWxF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPve9yWFs5b+7m3mdIhvXLB",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}